# 12-数据集成

数据集成就是将多个数据源合并存放在一个数据存储中（如数据仓库），从而方便后续的数据挖掘工作。

据统计，大数据项目中 80% 的工作都和数据集成有关，这里的数据集成有更广泛的意义，包括数据清洗、数据抽取、数据集成和数据变换等操作。因为数据挖掘前，我们需要的数据往往分布在不同的数据源中，需要考虑字段表达是否一样，以及属性是否冗余。

### 数据集成的两种架构：ETL和ELT

数据集成是数据工程师要做的工作之一。

ETL 是英文 Extract、Transform 和 Load 的缩写，顾名思义它包括了数据抽取、转换、加载三个过程。

抽取是将数据从已有的数据源中提取出来。

转换是对原始数据进行处理，例如将多个表进行连接构建一张新的表。

根据转换发生的顺序和位置，数据集成可以分为 ETL 和 ELT 两种架构。

ETL 的过程为提取 (Extract)——转换 (Transform)——加载 (Load)，在数据源抽取后首先进行转换，然后将转换的结果写入目的地。

ELT 的过程则是提取 (Extract)——加载 (Load)——变换 (Transform)，在抽取后将结果先写入目的地，然后利用数据库的聚合分析能力或者外部计算框架，如 Spark 来完成转换的步骤。

目前数据集成的主流架构是 ETL，但未来使用 ELT 作为数据集成架构的将越来越多。这样的好处：

* ELT 和 ETL 相比，最大的区别是“重抽取和加载，轻转换”，从而可以用更轻量的方案搭建起一个数据集成平台。使用 ELT 方法，在提取完成之后，数据加载会立即开始。一方面更省时，另一方面 ELT 允许 BI 分析人员无限制地访问整个原始数据，为分析师提供了更大的灵活性，使之能更好地支持业务。
* 在 ELT 架构中，数据变换这个过程根据后续使用的情况，需要在 SQL 中进行，而不是在加载阶段进行。这样做的好处是你可以从数据源中提取数据，经过少量预处理后进行加载。这样的架构更简单，使分析人员更好地了解原始数据的变换过程。

典型的 ETL 工具:

* 商业软件：Informatica PowerCenter、IBM InfoSphere DataStage、Oracle Data Integrator、Microsoft SQL Server Integration Services 等
* 开源软件：Kettle、Talend、Apatar、Scriptella、DataX、Sqoop 等

### Kettle 工具

Kettle 国外开源的ETL工具，中文名称叫水壶，该项目的目标是将各种数据放到一个壶里，然后以一种指定的格式流出。

Kettle 采用可视化的方式对数据库间的数据进行迁移。它包括了两种脚本：Transformation 转换和 Job 作业。

* Transformation（转换）：相当于一个容器，对数据操作进行了定义。数据操作就是数据从输入到输出的一个过程。你可以把转换理解成为是比作业粒度更小的容器。在通常的工作中，我们会把任务分解成为不同的作业，然后再把作业分解成多个转换。
  * Transformation 可以分成三个步骤，它包括了输入、中间转换以及输出。
  * 在 Transformation 中包括两个主要概念：Step 和 Hop。
    * Step 的就是步骤，Hop 就是跳跃线的意思。Step（步骤）：Step 是转换的最小单元，每一个 Step 完成一个特定的功能。在上面这个转换中，就包括了表输入、值映射、去除重复记录、表输出这 4 个步骤；
    * Hop（跳跃线）：用来在转换中连接 Step。它代表了数据的流向。
* Job（作业）：相比于转换是个更大的容器，它负责将转换组织起来完成某项作业。
  * 完整的任务，实际上是将创建好的转换和作业串联起来。在这里 Job 包括两个概念：Job Entry、Hop。
    * Job Entry（工作实体）：Job Entry 是 Job 内部的执行单元，每一个 Job Entry 都是用来执行具体的任务，比如调用转换，发送邮件等。
    * Hop：指连接 Job Entry 的线。并且它可以指定是否有条件地执行。

### DATAX

DataX 可以实现跨平台、跨数据库、不同系统之间的数据同步及交互，它将自己作为标准，连接不同的数据源，以完成它们之间的转换。

DataX 的模式基于框架 + 插件完成

在DataX框架里，Job 作业被 Splitter 分割器分成了许多小作业 Sub-Job。通过两个线程缓冲池来完成读和写的操作，读和写都是通过 Storage 完成数据的交换。比如在“读”模块，切分后的小作业，将数据从源头装载到 DataXStorage，然后在“写”模块，数据从 DataXStorage 导入到目的地。

好处：在整体的框架下，我们可以对 Reader 和 Writer 进行插件扩充，比如从 MySQL 导入到 Oracle，就可以使用 MySQLReader 和 OracleWriter 插件，装在框架上使用即可。

### Apache Sqoop

Sqoop 在 Hadoop 生态系统中在占据一席之地，它主要用来在 Hadoop 和关系型数据库中传递数据。

通过 Sqoop可以方便地将数据从关系型数据库导入到 HDFS 中，或者将数据从 HDFS 导出到关系型数据库中。

Hadoop 实现了一个分布式文件系统，即 HDFS。

Hadoop 的框架最核心的设计就是 HDFS 和 MapReduce。

HDFS 为海量的数据提供了存储，而 MapReduce 则为海量的数据提供了计算。





数据集成是将多源（多系统）、多样（结构化、非结构化、半结构化）、多维度数据整合进数据仓库，形成数据海洋，更好的提供业务分析系统的数据服务，通过数仓的数据集成，达到数据共享的效果，降低对原始业务系统的影响，同时加快数据分析工作者的数据准备周期。

数据集成最开始就是原始系统的数据，照样搬到数据仓库，这种类型工作长期实施，容易疲劳失去兴趣，理解业务需求，通过自己的数据集成、清洗、数据分析，提供有意思的数据，就是挖金子过程，应该也是一件有趣的事情。