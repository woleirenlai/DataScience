# 17-18-19-决策树

## 17-决策树

在现实生活遇到的各种选择，都是基于以往的经验来做判断。如果把判断背后的逻辑整理成一个结构图，会发现它实际上是一个树状图，这就是决策树。

### 决策树的工作原理

决策树基本上就是把以前的经验总结出来。比如要去打篮球，会根据一些条件来判断，最后得到结果是去还是不去。

![1](https://github.com/woleirenlai/Images/blob/master/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200418203643.png)

在做决策树的时候，会经历两个阶段：构造和剪枝。

**构造**

构造是生成一颗完整的决策树，**构造的过程是选择什么属性作为节点的过程**，构造的过程会存在三种节点。

1.根节点，是树的最顶端，最开始的节点。  
2.内部节点，是树中间的那些节点。  
3.叶节点，是树最底部的节点，也就是决策结果。

节点之间存在父子关系。根节点会有子节点，子节点会有子子节点，但是到了叶节点就停止了，叶节点不存在子节点。在构建过程中，需要解决三个重要问题。

1.选择哪个属性作为根节点。  
2.选择哪些属性作为子节点。  
3.什么时候停止并得到目标状态，也就是根节点。

**剪枝**

决策树构造出来之后可能还需要对决策树进行剪枝。剪枝就是给决策树瘦身，这一步想实现的目标就是，不需要太多的判断，同样可以得到不错的结果。这么做的目的是为了防止“过拟合”（Overfitting）。

过拟合就是指模型训练结果太好了，以至于在实际应用过程，会存在“死板”的情况，导致分类错误。

欠拟合和过拟合，左和右

![d30bfa3954ffdf5](https://github.com/woleirenlai/Images/blob/master/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/d30bfa3954ffdf5baf47ce53df9366df.jpg)

造成过拟合的原因之一就是因为训练集中样本量较小。如果决策树选择的属性过多，构造出来的决策树一定能够“完美”地把训练集中的样本分类，但是这样就会把训练集中一些数据的特点当成所有数据的特点，但这个特点不一定是全部数据的特点，这就使得这个决策树在真实的数据分类中出现错误，也就是模型的“泛化能力”差。

泛化能力指的分类器是通过训练集抽象出来的分类能力，可以理解是举一反三的能力。  
如果太依赖于训练集的数据，那么得到的决策树容错率就会比较低，泛化能力差。因为训练集只是全部数据的抽样，并不能体现全部数据的特点。

剪枝的方法：“预剪枝”（Pre-Pruning）和“后剪枝”（Post-Pruning）。

预剪枝是在决策树构造时就进行剪枝。  
方法是：构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为叶节点，不对其进行划分。

后剪枝就是在生成决策树之后再进行剪枝，通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。  
方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。

### 如何使用决策树做决策

例如一个打篮球的数据集，训练数据：

![17-2](https://github.com/woleirenlai/Images/blob/master/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/17-2.png)

在决策过程中有三个重要的问题：  
1.将哪个属性作为根节点？  
2.选择哪些属性作为后继节点？  
3.什么时候停止并得到目标值？

将那些属性作为根节点是关键问题。

有两个指标：纯度和信息熵

**纯度**：构造决策树的过程可以理解成为寻找纯净划分的过程。数学上用纯度来表示，换一种方式来解释就是让目标变量的分歧最小。

比如三个集合：

集合 1：6 次都去打篮球；  
集合 2：4 次去打篮球，2 次不去打篮球；  
集合 3：3 次去打篮球，3 次不去打篮球。

按照纯度指标来说，集合 1> 集合 2> 集合 3。因为集合 1 的分歧最小，集合 3 的分歧最大。

**信息熵**（entropy）表示**信息的不确定度**。

在信息论中，随机离散事件出现的概率存在着不确定性。为了衡量这种信息的不确定性，信息学之父香农引入了信息熵的概念，对应的数学公式：

![17-3](https://github.com/woleirenlai/Images/blob/master/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/17-3.png)

p(i|t) 代表了节点 t 为分类 i 的概率，其中 log2 为取以 2 为底的对数。

信息熵是一种度量，能帮我们反映出这个信息的不确定程度，当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。

比如有两个集合：

集合 1：5 次去打篮球，1 次不去打篮球；  
集合 2：3 次去打篮球，3 次不去打篮球。

在集合 1 中，有 6 次决策，其中打篮球是 5 次，不打篮球是 1 次。  
假设：类别 1 为“打篮球”，即次数为 5；类别 2 为“不打篮球”，即次数为 1。  
则节点划分为类别 1 的概率是 5/6，为类别 2 的概率是 1/6，带入公式可以计算得出：

![17-4](https://github.com/woleirenlai/Images/blob/master/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/17-4.png)

同理集合 2 中也是一共 6 次决策，其中类别 1 中“打篮球”的次数是 3，类别 2“不打篮球”的次数也是 3，则信息熵为：

![17-5](https://github.com/woleirenlai/Images/blob/master/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/17-5.png)

可以看出信息熵越大，纯度越低。当集合中的所有样本均匀混合时，信息熵最大，纯度最低。

在构造决策树的时会基于纯度来构建。而经典的 “不纯度”的指标有三种，分别是信息增益（ID3 算法）、信息增益率（C4.5 算法）以及基尼指数（Cart 算法）。

**ID3 算法**计算的是信息增益。  
**信息增益指的是划分可以带来纯度的提高，信息熵的下降**。其计算公式是父亲节点的信息熵减去所有子节点的信息熵。

在计算的过程中会计算每个子节点的归一化信息熵，即按照每个子节点在父节点中出现的概率，来计算这些子节点的信息熵。所以信息增益的公式可以表示为：

![17-6](https://github.com/woleirenlai/Images/blob/master/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/17-6.png)

其中 D 是父节点，Di 是子节点，Gain(D,a) 中的 a 作为 D 节点的属性选择。  

假设天气 = 晴时，会有 5 次去打篮球，5 次不打篮球。其中 D1 刮风 = 是，有 2 次打篮球，1 次不打篮球。D2 刮风 = 否，有 3 次打篮球，4 次不打篮球。则 a 代表节点的属性，即天气 = 晴。

![17-7](https://github.com/woleirenlai/Images/blob/master/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/17-7.jpg)

图中的例子，D 作为节点的信息增益为：

![17-8](https://github.com/woleirenlai/Images/blob/master/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/17-8.png)

即 D 节点的信息熵 - 2 个子节点的归一化信息熵。  
2 个子节点归一化信息熵 = 3/10 的 D1 信息熵 + 7/10 的 D2 信息熵。

基于 ID3 的算法规则，完整地计算训练集，训练集中一共有 7 条数据，3 个打篮球，4 个不打篮球，所以根节点的信息熵是：

![17-9](https://github.com/woleirenlai/Images/blob/master/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/17-9.png)

如果将天气作为属性的划分，会有三个叶子节点 D1、D2 和 D3，分别对应的是晴天、阴天和小雨。

用 + 代表去打篮球，- 代表不去打篮球。那么第一条记录，晴天不去打篮球，可以记为 1-，可以这样记录 D1，D2，D3：  
D1(天气 = 晴天) = {1-,2-,6+} →（第1条记录不去，第2条记录不去，第6条记录去）  
D2(天气 = 阴天) = {3+,7-}  
D3(天气 = 小雨) = {4+,5-}  
三个叶子节点的信息熵：

![17-10](https://github.com/woleirenlai/Images/blob/master/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/17-10.png)

D1 有 3 个记录，D2 有 2 个记录，D3 有 2 个记录，所以 D 中的记录一共是 3+2+2=7，即总数为 7。  
所以 D1 在 D（父节点）中的概率是 3/7，D2 在父节点的概率是 2/7，D3 在父节点的概率是 2/7。  
那么作为子节点的归一化信息熵 = 3/7\*0.918+2/7\*1.0*+2/7\*1.0=0.965。

用 ID3 中的信息增益来构造决策树，要计算每个节点的信息增益。

天气作为属性节点的信息增益为，Gain(D , 天气) = 0.985 - 0.965 = 0.020。

同理计算其他属性作为根节点的信息增益 ：  
Gain(D , 温度) = 0.128  
Gain(D , 湿度)=0.020  
Gain(D , 刮风)=0.020  
能看出温度作为属性的信息增益最大。因为 ID3 就是要将信息增益最大的节点作为父节点，这样可以得到纯度高的决策树，所以将温度作为根节点，其决策树状图分裂为：

![17-11](https://github.com/woleirenlai/Images/blob/master/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/17-11.jpg)

然后将图中第一个叶节点，即D1={1-,2-,3+,4+}进一步进行分裂，往下计算其不同属性（天气、湿度、刮风）作为节点的信息增益：  
Gain(D , 湿度)=1  
Gain(D , 天气)=1  
Gain(D , 刮风)=0.3115  
湿度或天气为 D1 的节点都可以得到最大的信息增益，选取湿度作为节点的属性划分。同理得到完整的决策树：

![17-12](https://github.com/woleirenlai/Images/blob/master/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/17-12.jpg)

ID3 的算法规则相对简单，可解释性强。但同样也存在缺陷：

有些属性可能对分类任务没有太大作用，但是他们仍然可能会被选为最优属性。 ID3 算法倾向于选择取值比较多的属性。如果把“编号”作为一个属性（一般情况下不会这么做，这里只是举个例子），那么“编号”将会被选为最优属性 。但实际上“编号”是无关属性的，它对“打篮球”的分类并没有太大作用。    
这种缺陷不是每次都会发生，只是存在一定的概率。在大部分情况下，ID3 都能生成不错的决策树分类。

**C4.5算法**

C4.5算法是ID3算法的改进，表现在：

**1.采用信息增益率**

ID3 在计算时倾向于选择取值多的属性。因此C4.5 采用信息增益率的方式来选择属性。

信息增益率 = 信息增益 / 属性熵。  
当属性有很多值的时候，相当于被划分成了许多份，虽然信息增益变大了，但是对于 C4.5 来说，属性熵也会变大，所以整体的信息增益率并不大。

**2.采用悲观剪枝**

ID3 算法容易产生过拟合。在 C4.5 中，会在决策树构造之后采用悲观剪枝（PEP），以提升决策树的泛化能力。  
悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。这种剪枝方法不再需要一个单独的测试数据集。

**3.离散化处理连续属性**

C4.5 可以对连续的属性进行离散化的处理。  
比如打篮球存在的“湿度”属性，不按照“高、中”划分，而是按照湿度值进行计算，那么湿度取什么值都有可能。**C4.5 选择具有最高信息增益的划分所对应的阈值。**

**4.处理缺失值**

假如原始数据如下，数据存在两点问题。  
1.数据集中存在数值缺失的情况，如何进行属性选择？  
2.假设已经做了属性划分，但是样本在这个属性上有缺失值，该如何对样本进行划分？

![17-13](https://github.com/woleirenlai/Images/blob/master/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/17-13.png)

不考虑缺失的数值，可以得到温度 D={2-,3+,4+,5-,6+,7-}。  
温度 = 高：D1={2-,3+,4+} ；  
温度 = 中：D2={6+,7-}；  
温度 = 低：D3={5-} 。  

\+ 号代表打篮球，- 号代表不打篮球。

比如 ID=2 时，决策是不打篮球，记录为 2-。针对将属性选择为温度的信息增为：

Gain(D′, 温度) = Ent(D′) - 0.792 = 1.0-0.792 = 0.208  
属性熵 =1.459，信息增益率 Gain_ratio(D′, 温度) = 0.208 / 1.459 = 0.1426。  
D′的样本个数为 6，而 D 的样本个数为 7，所以权重比例为 6/7，即 Gain(D′，温度) 所占权重比例为 6/7，所以：  
Gain_ratio(D, 温度)=6/7*0.1426=0.122。

这样即使在温度属性的数值有缺失的情况下，我们依然可以计算信息增益，并对属性进行选择。

对比一下：

ID3 算法的优点是方法简单，缺点是对噪声敏感。训练数据如果有少量错误，可能会产生决策树分类错误。

C4.5 在 ID3 的基础上用信息增益率代替了信息增益，解决了噪声敏感的问题，并且可以对构造树进行剪枝、处理连续数值以及数值缺失等情况，但是由于 C4.5 需要对数据集进行多次扫描，算法效率相对较低。