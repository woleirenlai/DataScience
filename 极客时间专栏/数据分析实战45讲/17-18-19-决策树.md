# 17-18-19-决策树

## 17-决策树

在现实生活遇到的各种选择，都是基于以往的经验来做判断。如果把判断背后的逻辑整理成一个结构图，会发现它实际上是一个树状图，这就是决策树。

### 决策树的工作原理

决策树基本上就是把以前的经验总结出来。比如要去打篮球，会根据一些条件来判断，最后得到结果是去还是不去。

![1](https://github.com/woleirenlai/Images/blob/master/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200418203643.png)

在做决策树的时候，会经历两个阶段：构造和剪枝。

**构造**

构造是生成一颗完整的决策树，**构造的过程是选择什么属性作为节点的过程**，构造的过程会存在三种节点。

1.根节点，是树的最顶端，最开始的节点。  
2.内部节点，是树中间的那些节点。  
3.叶节点，是树最底部的节点，也就是决策结果。

节点之间存在父子关系。根节点会有子节点，子节点会有子子节点，但是到了叶节点就停止了，叶节点不存在子节点。在构建过程中，需要解决三个重要问题。

1.选择哪个属性作为根节点。  
2.选择哪些属性作为子节点。  
3.什么时候停止并得到目标状态，也就是根节点。

**剪枝**

决策树构造出来之后可能还需要对决策树进行剪枝。剪枝就是给决策树瘦身，这一步想实现的目标就是，不需要太多的判断，同样可以得到不错的结果。这么做的目的是为了防止“过拟合”（Overfitting）。

过拟合就是指模型训练结果太好了，以至于在实际应用过程，会存在“死板”的情况，导致分类错误。

欠拟合和过拟合，左和右

![d30bfa3954ffdf5](https://github.com/woleirenlai/Images/blob/master/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/d30bfa3954ffdf5baf47ce53df9366df.jpg)

造成过拟合的原因之一就是因为训练集中样本量较小。如果决策树选择的属性过多，构造出来的决策树一定能够“完美”地把训练集中的样本分类，但是这样就会把训练集中一些数据的特点当成所有数据的特点，但这个特点不一定是全部数据的特点，这就使得这个决策树在真实的数据分类中出现错误，也就是模型的“泛化能力”差。

泛化能力指的分类器是通过训练集抽象出来的分类能力，可以理解是举一反三的能力。  
如果太依赖于训练集的数据，那么得到的决策树容错率就会比较低，泛化能力差。因为训练集只是全部数据的抽样，并不能体现全部数据的特点。

剪枝的方法：“预剪枝”（Pre-Pruning）和“后剪枝”（Post-Pruning）。

预剪枝是在决策树构造时就进行剪枝。  
方法是：构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为叶节点，不对其进行划分。

后剪枝就是在生成决策树之后再进行剪枝，通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。  
方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。

### 如何使用决策树做决策

